# H∆∞·ªõng D·∫´n T√≠ch H·ª£p Qwen Fine-tuned Model

## üéØ T·∫°i Sao Fine-tune Qwen KH√îNG V√¥ Nghƒ©a?

**Fine-tune Qwen c·ªßa b·∫°n C√ì TH·ªÇ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ thay th·∫ø GPT-4o-mini cho:**

1. ‚úÖ **Intent Classification** - Ph√¢n lo·∫°i √Ω ƒë·ªãnh ng∆∞·ªùi d√πng
2. ‚úÖ **Response Generation** - Sinh c√¢u tr·∫£ l·ªùi t·ª± nhi√™n
3. ‚úÖ **Ti·∫øt ki·ªám chi ph√≠** - Kh√¥ng c·∫ßn tr·∫£ ph√≠ API OpenAI
4. ‚úÖ **Privacy** - D·ªØ li·ªáu kh√¥ng g·ª≠i l√™n cloud
5. ‚úÖ **T·ªëc ƒë·ªô** - C√≥ th·ªÉ nhanh h∆°n n·∫øu c√≥ GPU

---

## üìä So S√°nh: Qwen vs GPT-4o-mini

| Ti√™u ch√≠ | Qwen 3B (Fine-tuned) | GPT-4o-mini |
|----------|---------------------|-------------|
| **Chi ph√≠** | Mi·ªÖn ph√≠ (local) | ~$0.15/1M tokens |
| **Privacy** | ‚úÖ 100% local | ‚ùå G·ª≠i l√™n OpenAI |
| **T·ªëc ƒë·ªô** | ~200-500ms (GPU) | ~500-2000ms (API) |
| **Ch·∫•t l∆∞·ª£ng** | T√πy fine-tune | R·∫•t t·ªët |
| **C·∫ßn GPU** | ‚úÖ Khuy·∫øn ngh·ªã | ‚ùå Kh√¥ng c·∫ßn |
| **Fine-tune** | ‚úÖ ƒê√£ fine-tune | ‚ùå Kh√¥ng th·ªÉ |

---

## üöÄ C√°ch T√≠ch H·ª£p Qwen V√†o H·ªá Th·ªëng

### Option 1: Thay th·∫ø ho√†n to√†n GPT-4o-mini

**L·ª£i √≠ch:**
- ‚úÖ Kh√¥ng c·∫ßn OpenAI API key
- ‚úÖ Privacy ho√†n to√†n
- ‚úÖ Ti·∫øt ki·ªám chi ph√≠

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå C·∫ßn GPU ƒë·ªÉ ch·∫°y nhanh
- ‚ùå Ch·∫•t l∆∞·ª£ng c√≥ th·ªÉ k√©m h∆°n GPT-4o-mini (t√πy fine-tune)

### Option 2: Hybrid (Qwen + GPT)

**L·ª£i √≠ch:**
- ‚úÖ Qwen cho Intent (nhanh, r·∫ª)
- ‚úÖ GPT cho Response (ch·∫•t l∆∞·ª£ng cao)
- ‚úÖ Fallback n·∫øu Qwen l·ªói

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå V·∫´n c·∫ßn OpenAI API key
- ‚ùå Ph·ª©c t·∫°p h∆°n

### Option 3: Ch·ªâ d√πng Qwen khi kh√¥ng c√≥ API key

**L·ª£i √≠ch:**
- ‚úÖ Fallback t·ª± ƒë·ªông
- ‚úÖ V·∫´n d√πng GPT khi c√≥ API key

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Ch·∫•t l∆∞·ª£ng kh√¥ng ƒë·ªìng nh·∫•t

---

## üíª Implementation Plan

### B∆∞·ªõc 1: T·∫°o Qwen Service trong Python

T·∫°o endpoint m·ªõi trong `retrieval_service/main.py`:

```python
@app.post("/qwen/intent")
def qwen_classify_intent(req: QwenIntentRequest) -> QwenIntentResponse:
    """Classify intent using Qwen model."""
    # Load Qwen model
    # Generate response
    # Return intent
    pass

@app.post("/qwen/generate")
def qwen_generate_response(req: QwenGenerateRequest) -> QwenGenerateResponse:
    """Generate assistant message using Qwen model."""
    # Load Qwen model
    # Generate response
    # Return message
    pass
```

### B∆∞·ªõc 2: Update Frontend API Route

Trong `app/api/search/route.ts`:

```typescript
// Option: Use Qwen if available, fallback to GPT
async function classifyIntent(q: string): Promise<Intent> {
  // Try Qwen first
  try {
    const qwenRes = await fetch("http://127.0.0.1:8000/qwen/intent", {
      method: "POST",
      body: JSON.stringify({ query: q }),
    });
    if (qwenRes.ok) {
      const data = await qwenRes.json();
      return data.intent;
    }
  } catch {
    // Fallback to GPT
  }
  
  // Use GPT-4o-mini as fallback
  return classifyIntentWithGPT(q);
}
```

---

## üîß Code Example: Qwen Integration

### 1. Load Qwen Model (trong retrieval_service/main.py)

```python
_qwen_model: Any | None = None
_qwen_tokenizer: Any | None = None

def _load_qwen_llm() -> Tuple[Any, Any]:
    """Load Qwen 3B for LLM tasks (intent, generation)."""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    
    adapter_path = _get_adapter_path()
    config_path = adapter_path / "adapter_config.json"
    
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    base = cfg.get("base_model_name_or_path", "Qwen/Qwen2.5-3B-Instruct")
    
    logger.info(f"[QWEN] Loading base model: {base}")
    tokenizer = AutoTokenizer.from_pretrained(base, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        base,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
    )
    
    logger.info(f"[QWEN] Attaching LoRA adapter...")
    model = PeftModel.from_pretrained(model, str(adapter_path))
    model.eval()
    
    return model, tokenizer
```

### 2. Intent Classification v·ªõi Qwen

```python
def _qwen_classify_intent(query: str) -> str:
    """Classify intent using Qwen model."""
    if not _qwen_model or not _qwen_tokenizer:
        return "unknown"
    
    prompt = f"""<|im_start|>system
B·∫°n l√† b·ªô ph√¢n lo·∫°i intent cho m·ªôt TR·ª¢ L√ù T∆Ø V·∫§N TH·ªúI TRANG. 
H√£y ph√¢n lo·∫°i c√¢u c·ªßa user v√†o ƒë√∫ng M·ªòT trong 3 nh√≥m:
- fashion (t√¨m ki·∫øm / t∆∞ v·∫•n s·∫£n ph·∫©m qu·∫ßn √°o, gi√†y d√©p, ph·ª• ki·ªán th·ªùi trang)
- chitchat (ch√†o h·ªèi, n√≥i chuy·ªán phi·∫øm, c√¢u th√¢n m·∫≠t kh√¥ng c·∫ßn g·ª£i √Ω s·∫£n ph·∫©m)
- off_topic (ch·ªß ƒë·ªÅ kh√¥ng li√™n quan ƒë·∫øn th·ªùi trang)

Ch·ªâ tr·∫£ l·ªùi duy nh·∫•t m·ªôt t·ª´: fashion, chitchat, ho·∫∑c off_topic.
<|im_end|>
<|im_start|>user
{query}
<|im_end|>
<|im_start|>assistant
"""
    
    inputs = _qwen_tokenizer(prompt, return_tensors="pt")
    device = next(_qwen_model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = _qwen_model.generate(
            **inputs,
            max_new_tokens=4,
            temperature=0.1,
            do_sample=False,
        )
    
    response = _qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    intent = response.split("assistant")[-1].strip().lower()
    
    if "fashion" in intent:
        return "fashion"
    elif "chitchat" in intent:
        return "chitchat"
    elif "off_topic" in intent or "offtopic" in intent:
        return "off_topic"
    return "unknown"
```

### 3. Response Generation v·ªõi Qwen

```python
def _qwen_generate_response(query: str, intent: str, has_products: bool) -> str:
    """Generate assistant message using Qwen model."""
    if not _qwen_model or not _qwen_tokenizer:
        return ""
    
    prompt = f"""<|im_start|>system
B·∫°n l√† m·ªôt TR·ª¢ L√ù T∆Ø V·∫§N TH·ªúI TRANG n√≥i ti·∫øng Vi·ªát.
QUAN TR·ªåNG: B·∫°n CH·ªà ƒë∆∞·ª£c n√≥i v·ªÅ s·∫£n ph·∫©m c√≥ trong h·ªá th·ªëng database.
KH√îNG BAO GI·ªú ƒë∆∞·ª£c ƒë∆∞a ra t√™n c·ª≠a h√†ng, th∆∞∆°ng hi·ªáu, ho·∫∑c b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ trong database.
Lu√¥n tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n, ch·ªâ d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø c√≥ trong h·ªá th·ªëng.
<|im_end|>
<|im_start|>user
Intent: {intent}
C√≥ t√¨m ƒë∆∞·ª£c s·∫£n ph·∫©m? {"C√≥" if has_products else "Kh√¥ng"}
C√¢u ng∆∞·ªùi d√πng: "{query}"
<|im_end|>
<|im_start|>assistant
"""
    
    inputs = _qwen_tokenizer(prompt, return_tensors="pt")
    device = next(_qwen_model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = _qwen_model.generate(
            **inputs,
            max_new_tokens=120,
            temperature=0.5,
            do_sample=True,
        )
    
    response = _qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    message = response.split("assistant")[-1].strip()
    return message
```

---

## üìà Performance Considerations

### GPU Requirements

- **CPU only**: ~2-5 gi√¢y/request (ch·∫≠m)
- **GPU (CUDA)**: ~200-500ms/request (nhanh)
- **GPU (MPS - Apple Silicon)**: ~300-600ms/request

### Memory Requirements

- **Qwen 3B**: ~6-8GB RAM/VRAM
- **LoRA adapter**: +1-2GB

### Optimization Tips

1. **Quantization**: D√πng 8-bit ho·∫∑c 4-bit quantization ƒë·ªÉ gi·∫£m memory
2. **Batch Processing**: X·ª≠ l√Ω nhi·ªÅu requests c√πng l√∫c
3. **Caching**: Cache model trong memory ƒë·ªÉ tr√°nh reload

---

## üéØ Recommendation

### N·∫øu b·∫°n c√≥ GPU:
‚úÖ **N√™n d√πng Qwen** cho c·∫£ Intent v√† Response Generation
- Ti·∫øt ki·ªám chi ph√≠
- Privacy t·ªët
- T·ªëc ƒë·ªô ch·∫•p nh·∫≠n ƒë∆∞·ª£c

### N·∫øu kh√¥ng c√≥ GPU:
‚ö†Ô∏è **C√¢n nh·∫Øc**:
- D√πng Qwen cho Intent (ch·∫•p nh·∫≠n ch·∫≠m)
- D√πng GPT cho Response (ch·∫•t l∆∞·ª£ng cao h∆°n)

### Hybrid Approach (Khuy·∫øn ngh·ªã):
‚úÖ **D√πng c·∫£ 2**:
- Qwen l√†m primary (khi c√≥ GPU)
- GPT l√†m fallback (khi Qwen l·ªói ho·∫∑c kh√¥ng c√≥ GPU)

---

## üîÑ Migration Path

1. **Phase 1**: Th√™m Qwen endpoints v√†o Python service
2. **Phase 2**: Update frontend ƒë·ªÉ d√πng Qwen (v·ªõi fallback GPT)
3. **Phase 3**: Test v√† so s√°nh ch·∫•t l∆∞·ª£ng
4. **Phase 4**: Quy·∫øt ƒë·ªãnh d√πng Qwen ho√†n to√†n hay hybrid

---

## ‚ùì FAQ

**Q: Fine-tune Qwen c√≥ t·ªët h∆°n GPT-4o-mini kh√¥ng?**
A: T√πy v√†o ch·∫•t l∆∞·ª£ng fine-tune. N·∫øu fine-tune t·ªët, c√≥ th·ªÉ t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c t·ªët h∆°n cho domain c·ª• th·ªÉ.

**Q: C√≥ c·∫ßn rebuild index kh√¥ng?**
A: Kh√¥ng. Qwen ch·ªâ d√πng cho LLM tasks (intent, generation), kh√¥ng li√™n quan ƒë·∫øn embedding/index.

**Q: C√≥ th·ªÉ d√πng Qwen cho embedding kh√¥ng?**
A: C√≥ th·ªÉ, nh∆∞ng e5-large-v2 t·ªët h∆°n cho embedding task.

**Q: Fine-tune Qwen c√≥ m·∫•t kh√¥ng?**
A: Kh√¥ng. Model v√† LoRA adapter v·∫´n c√≤n trong `epoch_2_final/`.

